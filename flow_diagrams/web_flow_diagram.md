# Web RAG Demo Flow Diagram

## Complete Flow Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    WEB RAG DEMO INITIALIZATION                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚
                                  â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  Import Dependencies     â”‚
                    â”‚  Load LLM Model          â”‚
                    â”‚  (llama3.2:3b)          â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚
                                  â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚ Vector DB Exists?        â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚              â”‚
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚                                  â”‚
                â–¼ YES                              â–¼ NO
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ Load Existing DB â”‚         â”‚ Create New Database    â”‚
        â”‚ Check for new    â”‚         â”‚ (web_vector.py steps)  â”‚
        â”‚ URLs & update    â”‚         â”‚ Scrape URLs            â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚                                  â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚
                                  â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚ Create Retriever             â”‚
                    â”‚ (similarity search, k=10)    â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         USER QUERY INPUT                             â”‚
â”‚            "Who identified dragon bones?"                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚
                                  â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  Choose Approach         â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚              â”‚
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚                                  â”‚
                â–¼                                  â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚   WITHOUT RAG    â”‚        â”‚     WITH RAG         â”‚
        â”‚                  â”‚        â”‚                      â”‚
        â”‚ Direct LLM Query â”‚        â”‚ 1. Retrieve Chunks   â”‚
        â”‚ (LLM only uses   â”‚        â”‚ 2. Build Context     â”‚
        â”‚  training data)  â”‚        â”‚ 3. LLM with Context  â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚                                  â”‚
                â–¼                                  â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  LLM-Only Response   â”‚    â”‚   RAG-Enhanced Response  â”‚
        â”‚ (General knowledge   â”‚    â”‚   (Fact-based, accurate) â”‚
        â”‚  may be inaccurate)  â”‚    â”‚                          â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚                                  â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚
                                  â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  Display Both Answers    â”‚
                    â”‚  for Comparison          â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## STEP 1: Web Vector Database Creation (web_vector.py)

### 1.1 Check if Vector DB Already Exists
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Does web_chroma_db/ exist?      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                    â”‚
      YESâ”‚                    â”‚NO
         â”‚                    â”‚
         â–¼                    â–¼
    Load DB              Scrape URLs
    (from disk)          from the internet
    Check for new            â”‚
    URLs & update        âœ“ Fetches web pages
         â”‚               âœ“ Extracts text
         â”‚               âœ“ Handles HTML/JS
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
              âœ“ Faster (reuse existing data)
              âœ“ Auto-update with new URLs
```

**Code:**
```python
# web_vector.py - Lines 79-110
if os.path.exists(persist_directory) and not force_rebuild:
    print(f"Loading existing vector DB from {persist_directory}")
    vectordb = Chroma(
        embedding_function=embeddings,
        persist_directory=persist_directory
    )
    existing_count = vectordb._collection.count()
    print(f"âœ“ Loaded existing DB with {existing_count} chunks")
    
    if auto_update:
        existing_sources = get_existing_sources(vectordb)
        new_urls = [url for url in urls if url not in existing_sources]
        
        if new_urls:
            print(f"\nğŸ” Found {len(new_urls)} new URL(s) to scrape")
```

---

### 1.2 Fetch Web Pages from URLs
```
URLs List
   â”‚
   â”œâ”€â–º https://en.wikipedia.org/wiki/Dinosaur
   â”œâ”€â–º https://en.wikipedia.org/wiki/Paleontology
   â”œâ”€â–º https://en.wikipedia.org/wiki/Fossil
   â”‚
   â–¼
WebBaseLoader
   â”‚
   â”œâ”€â–º Sends HTTP requests to URLs
   â”œâ”€â–º Fetches HTML content
   â”œâ”€â–º Extracts readable text
   â”œâ”€â–º Handles JavaScript content
   â”œâ”€â–º Cleans up formatting
   â”‚
   â–¼
List of Document Objects
   â”‚
   â”œâ”€â–º Document 1: Wikipedia page on Dinosaur
   â”œâ”€â–º Document 2: Wikipedia page on Paleontology
   â”œâ”€â–º Document 3: Wikipedia page on Fossil
   â”‚
   â–¼
Total: N pages loaded
```

**Code:**
```python
# web_vector.py - Lines 12-28
def load_webpages(urls):
    """Load web pages using LangChain's WebBaseLoader."""
    try:
        print(f"Loading {len(urls)} URLs with WebBaseLoader...")
        loader = WebBaseLoader(urls)
        documents = loader.load()
        print(f"âœ“ Successfully loaded {len(documents)} pages")
        return documents
    except Exception as e:
        print(f"âŒ Error loading URLs: {e}")
        return []
```

**Example Output:**
```
Loading 1 URLs with WebBaseLoader...
âœ“ Successfully loaded 1 pages
```

---

### 1.3 Chunk Web Content
```
Web Page Content
   â”‚
   "Dinosaurs are extinct reptiles that lived
    millions of years ago. Fossils show...
    The first dinosaur bones were identified
    by Mary Anning in the 1800s..."
   â”‚
   â–¼
RecursiveCharacterTextSplitter
   â”‚
   â”œâ”€ Chunk size: 1000 characters
   â”œâ”€ Overlap: 200 characters
   â”œâ”€ Split intelligently at:
   â”‚  â”œâ”€ Double newlines (paragraphs)
   â”‚  â”œâ”€ Single newlines (sentences)
   â”‚  â”œâ”€ Spaces (words)
   â”‚  â””â”€ Characters (fallback)
   â”‚
   â–¼
Multiple Chunks
   â”‚
   â”œâ”€â–º Chunk 1: "Dinosaurs are extinct..." (overlap ends here)
   â”œâ”€â–º Chunk 2: "...from 1800s. The fossil record..." (overlap starts from chunk 1)
   â”œâ”€â–º Chunk 3: "record shows dragon bones..."
   â”‚
   â–¼
Chunked Documents (overlap preserves context)
```

**Code:**
```python
# web_vector.py - Lines 30-47
def chunk_documents(documents, chunk_size=1000, chunk_overlap=200):
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap,
        length_function=len,
        separators=["\n\n", "\n", " ", ""]
    )
    return text_splitter.split_documents(documents)
```

**Example:**
- 1 Wikipedia page â†’ ~15-30 chunks
- Each chunk: max 1000 characters
- Overlap: 200 characters between chunks

---

### 1.4 Initialize Embedding Model
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Ollama Embedding Model         â”‚
â”‚ mxbai-embed-large:335m         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
    Ready to convert web chunks
    to 335-dimensional vectors
```

**Code:**
```python
# web_vector.py - Lines 138-139
embeddings = OllamaEmbeddings(model=EMBEDDING_MODEL)
print("âœ“ Embedding model initialized")
```

---

### 1.5 Generate Embeddings and Store in ChromaDB
```
Chunked Web Content: "Dinosaurs are extinct..."
   â”‚
   â–¼
Embedding Model
   â”‚
   â–¼
Vector: [0.234, -0.156, 0.890, ..., 0.123]  (335 dimensions)
   â”‚
   â”œâ”€â–º Store in ChromaDB
   â”œâ”€â–º Link to metadata (source URL)
   â””â”€â–º Save to disk (web_chroma_db/)

[Repeat for all chunks]
   â”‚
   â–¼
ChromaDB fully populated with vectors
Ready for similarity search
```

**Code:**
```python
# web_vector.py - Lines 141-147
vectordb = Chroma.from_documents(
    documents=chunked_docs,
    embedding=embeddings,
    persist_directory=persist_directory
)
print(f"âœ“ Vector DB created and saved to {persist_directory}")
print(f"  Total chunks embedded: {len(chunked_docs)}")
```

**Metadata Stored:**
```python
{
  "source": "https://en.wikipedia.org/wiki/Dinosaur",
  "title": "Dinosaur - Wikipedia"
}
```

---

## STEP 2: Query Processing - Without RAG (web_main.py)

```
User Question
  â”‚
  "Who identified dragon bones?"
  â”‚
  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ LLM Model (llama3.2:3b)          â”‚
â”‚ WITHOUT any web content context  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â”‚
  â”œâ”€â–º Uses only training data
  â”œâ”€â–º No access to actual web pages
  â”œâ”€â–º May have outdated information
  â””â”€â–º Limited historical accuracy
  â”‚
  â–¼
"Dragon bones were likely identified 
by ancient Chinese scholars who 
confused them with mythical dragons. 
Modern paleontologists later..."
(Vague, possibly inaccurate)
```

**Code:**
```python
# web_main.py - Lines 15-20
def ask_without_rag(question: str) -> str:
    """Ask the LLM directly without vector database context."""
    prompt = f"You are a knowledgeable assistant. Answer the following question:\n\nQuestion: {question}\n\nAnswer:"
    return model.invoke(prompt)
```

---

## STEP 3: Query Processing - With RAG (web_main.py)

### 3.1 Retrieve Relevant Web Content Chunks
```
User Question
  â”‚
  "Who identified dragon bones?"
  â”‚
  â–¼
Convert to Embedding
(same model: mxbai-embed-large:335m)
  â”‚
  â–¼
Vector: [0.145, -0.234, 0.567, ..., 0.890]
  â”‚
  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ChromaDB Similarity Search        â”‚
â”‚ Find top 10 most similar chunks  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â”‚
  â”œâ”€â–º Chunk 1: "Mary Anning identified fossils called dragon bones..." - Similarity: 0.98
  â”œâ”€â–º Chunk 2: "In the 1800s paleontology emerged..." - Similarity: 0.93
  â”œâ”€â–º Chunk 3: "Dragon bones were actually dinosaur bones..." - Similarity: 0.91
  â”œâ”€â–º ... (more chunks)
  â”‚
  â–¼
Return top 10 chunks with metadata
   (source URL, exact text)
```

**Code:**
```python
# web_main.py - Lines 35-36
retrieved_docs = retriever_web.invoke(question)
```

---

### 3.2 Build Context Prompt with Retrieved Web Chunks
```
Retrieved Chunks (10 web page sections)
  â”‚
  â”œâ”€â–º Chunk 1: Historical facts from Wikipedia
  â”œâ”€â–º Chunk 2: Scientific discovery info
  â”œâ”€â–º Chunk 3: Paleontology details
  â”œâ”€â–º ... (more chunks with actual web content)
  â”‚
  â–¼
Format into prompt:
  â”‚
  "You are a knowledgeable assistant.
   
   Here are the relevant web pages:
   
   === Web Page 1 ===
   Mary Anning was a pioneer of paleontology
   who discovered many fossils in the 1800s.
   Fossils that were originally called 'dragon bones'
   in ancient times were identified as...
   
   === Web Page 2 ===
   The term 'dragon bones' was used historically
   to refer to what we now know as dinosaur fossils.
   Modern paleontologists identified these...
   
   Question: Who identified dragon bones?
   
   Provide a clear answer based on the web content above:"
  â”‚
  â–¼
Full context-rich prompt ready for LLM
```

**Code:**
```python
# web_main.py - Lines 40-46
prompt = f"You are a knowledgeable assistant. Answer the following question based on the web content provided.\n\n"
prompt += f"Here are the relevant web pages:\n\n"

for i, doc in enumerate(retrieved_docs, 1):
    prompt += f"=== Web Page {i} ===\n{doc.page_content}\n\n"

prompt += f"Question: {question}\n\nProvide a clear answer based on the web content above:"
```

---

### 3.3 Generate RAG-Enhanced Answer
```
LLM Model (with web content)
  â”‚
  â”œâ”€â–º Sees actual web page content
  â”œâ”€â–º Can reference Wikipedia facts
  â”œâ”€â–º Provides accurate, sourced information
  â””â”€â–º Backed by current web sources
  â”‚
  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ RAG-ENHANCED RESPONSE                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â”‚
  "Based on the web content provided:
  
  Mary Anning, a pioneering paleontologist of the 1800s,
  was instrumental in identifying fossils that were
  historically referred to as 'dragon bones'. These were
  later scientifically identified as dinosaur fossils.
  
  The term 'dragon bones' originated from ancient times
  when these fossils were misidentified as belonging to
  mythical dragons before modern paleontology emerged."
  â”‚
  â–¼
(Accurate! Sourced from Wikipedia!)
```

---

## STEP 4: Web Data Pipeline Detailed View

### Data Transformation Pipeline
```
URLs to Scrape
   â”‚
   â”œâ”€â–º https://en.wikipedia.org/wiki/Dinosaur
   â”œâ”€â–º https://en.wikipedia.org/wiki/Paleontology
   â”‚
   â–¼
Step 1: Fetch Web Pages
   â”‚
   â”œâ”€â–º Send HTTP requests
   â”œâ”€â–º Extract readable text from HTML
   â”œâ”€â–º 2-3 pages loaded
   â”‚
   â–¼
Step 2: Chunk Content
   â”‚
   â”œâ”€â–º 1000 char chunks with 200 char overlap
   â”œâ”€â–º ~40-50 total chunks (from 2-3 pages)
   â”‚
   â–¼
Step 3: Generate Embeddings
   â”‚
   â”œâ”€â–º Convert each chunk to vector (335 dimensions)
   â”œâ”€â–º 40-50 vectors created
   â”‚
   â–¼
Step 4: Store in ChromaDB
   â”‚
   â”œâ”€â–º Save to web_chroma_db/
   â”œâ”€â–º Ready for queries
   â”‚
   â–¼
NEXT TIME: 
   â”œâ”€ Load from disk (< 1 second)
   â”œâ”€ Check for new URLs
   â”œâ”€ Add new URLs if needed
```

---

## Configuration

```yaml
Web Scraping:
  Loader: WebBaseLoader (LangChain)
  URLs: Wikipedia, news sites, etc.
  Note: Some sites block automated scraping
  Recommended: Wikipedia (usually scraping-friendly)
  
Chunking Strategy:
  Chunk size: 1000 characters
  Overlap: 200 characters
  Split at: Double newline, newline, space, character
  
Embedding:
  Model: mxbai-embed-large:335m
  Dimensions: 335 million parameters
  Size: ~335MB
  
LLM:
  Model: llama3.2:3b
  Size: ~2GB
  
Vector Database:
  Type: ChromaDB
  Backend: SQLite
  Location: web_chroma_db/
  Retrieval: Similarity search (cosine distance)
  
Query Parameters:
  Top K results: 10 chunks
  Search type: similarity
  
Auto-Update:
  Enabled: YES
  New URLs added automatically
  Existing data preserved
```

---

## Running the Web Demo

```bash
# Step 1: Create/update vector database from URLs
python web_vector.py

# Step 2: Run the comparison demo
python web_main.py
```

**Expected Output:**
```
Topic: DINOSAURS
URLs: 1 pages
Question: Who identified dragon bones?

WITHOUT RAG (Using LLM Knowledge Only)
================================================================================
[LLM-only response - vague about dragon bones]

================================================================================
WITH RAG (Using Web-Scraped Content)
================================================================================
[RAG-enhanced response - accurate from Wikipedia]
```

---

## Key Differences: Data Source Comparison

| Aspect | CSV Data | PDF Data | Web Data |
|--------|----------|----------|----------|
| **Source Type** | Structured records | Unstructured PDFs | Semi-structured HTML |
| **Processing** | Direct load | Extract + chunk | Scrape + extract + chunk |
| **Dynamic** | Static file | Static file | Can change over time |
| **Chunks** | One per record | Multiple per page | Multiple per page |
| **Auto-Update** | Check for files | Check for files | Check for URLs |
| **Reliability** | High (local) | High (local) | Medium (network dependent) |
| **Metadata** | Full columns | Page numbers | URL, title |
| **Chunking** | Not needed | Required | Required |

---

## Troubleshooting

### "Websites blocking requests"
```
Solution:
1. Try Wikipedia URLs (usually work)
2. Some sites block User-Agent requests
3. Use sites that allow scraping in their robots.txt
4. Add delays between requests if needed
```

### "Network connectivity issues"
```
Solution:
1. Check your internet connection
2. Verify URLs are accessible in browser
3. Try different URLs
4. Check if proxy is needed
```

### "Empty or partial content"
```
Solution:
1. Some websites use JavaScript (WebBaseLoader may not handle)
2. Try simpler, text-heavy pages
3. Wikipedia articles work well
4. Static HTML pages preferred over dynamic content
```

### "Too many or too few chunks"
```
Solution:
- Adjust CHUNK_SIZE (current: 1000)
- Adjust CHUNK_OVERLAP (current: 200)
- Larger chunks = fewer total chunks
- More overlap = better context but slower
```

---

## Running All Three Demos

```bash
# Step 1: Create all vector databases
python csv_vector.py
python pdf_vector.py
python web_vector.py

# Step 2: Run all three demos
python csv_main.py
python pdf_main.py
python web_main.py

# Compare results across different data sources!
```

This allows you to see how RAG improves answers across three different data source types:
- **Structured (CSV)**: Book records â†’ Great accuracy
- **Unstructured (PDF)**: Board game rules â†’ Rules-based accuracy
- **Semi-structured (Web)**: Wikipedia content â†’ General knowledge accuracy
